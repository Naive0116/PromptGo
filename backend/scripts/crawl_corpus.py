#!/usr/bin/env python3
"""
çˆ¬å– prompt-writing-corpus.md ä¸­çš„æ‰€æœ‰ URL å†…å®¹
å°†å†…å®¹ä¿å­˜åˆ°æœ¬åœ°å¹¶ç´¢å¼•åˆ°å‘é‡åº“
"""
import asyncio
import re
import os
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

import httpx

# è¯­æ–™åº“ URL åˆ—è¡¨ï¼ˆä» prompt-writing-corpus.md æå–ï¼‰
CORPUS_URLS = [
    # P0 - Prompt å·¥ç¨‹ä¸ç»“æ„åŒ–å†™æ³•
    {
        "id": "ANTHROPIC_PROMPT_OVERVIEW",
        "type": "doc",
        "priority": "P0",
        "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
        "topic": "prompting",
        "tags": ["prompting", "system_prompt", "best_practices", "claude"],
        "notes": "å®˜æ–¹è§†è§’çš„ prompt åŸºæœ¬åŸåˆ™ï¼šæ¸…æ™°ã€ç¤ºä¾‹ã€å¤šè½®ã€è§’è‰²ã€é•¿ä¸Šä¸‹æ–‡æŠ€å·§ç­‰"
    },
    {
        "id": "ANTHROPIC_XML_TAGS",
        "type": "doc",
        "priority": "P0",
        "url": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags",
        "topic": "prompting",
        "tags": ["prompting", "xml", "context_separation", "anti_injection"],
        "notes": "æŠŠ instructions / context / examples / output åˆ†ç¦»ï¼Œé™ä½æ•°æ®-æŒ‡ä»¤æ··æ·†"
    },
    {
        "id": "ANTHROPIC_METAPROMPT",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/anthropics/anthropic-cookbook/main/misc/metaprompt.ipynb",
        "topic": "prompting",
        "tags": ["metaprompt", "few_shot", "prompt_generator", "examples"],
        "notes": "å¤šæ ·ä¾‹ few-shotï¼šè®©æ¨¡å‹å­¦ä¼šæŠŠä»»åŠ¡å†™æˆé«˜è´¨é‡ prompt"
    },
    {
        "id": "LANGGPT_FRAMEWORK",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/langgptai/LangGPT/main/README.md",
        "topic": "prompting",
        "tags": ["langgpt", "structured_prompt", "modules", "compile"],
        "notes": "æŠŠ prompt å†™ä½œæ¨¡å—åŒ–ï¼šRole/Profile/Rules/Workflow/Initialization"
    },
    # P0 - äº§å©†æœ¯ / æ¨ç†å‹æç¤º
    {
        "id": "MAIEUTIC_PROMPTING",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2205.11822",
        "topic": "prompting",
        "tags": ["socratic", "maieutic", "clarify", "consistency"],
        "notes": "å…³é”®ä»·å€¼ï¼šä»ä¸å®Œç¾è§£é‡Šä¸­åšä¸€è‡´æ€§å½’çº³ï¼ˆé€‚åˆè¿½é—®â†’è‡ªè¯â†’æ”¶æ•›ï¼‰"
    },
    {
        "id": "CHAIN_OF_THOUGHT",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2201.11903",
        "topic": "prompting",
        "tags": ["cot", "reasoning", "self_check"],
        "notes": "é€‚åˆåœ¨ç”Ÿæˆå™¨å†…éƒ¨åšä¸ºä»€ä¹ˆè¿™æ ·å†™çš„è‡ªæ£€"
    },
    {
        "id": "SELF_CONSISTENCY",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2203.11171",
        "topic": "prompting",
        "tags": ["self_consistency", "sampling", "selection"],
        "notes": "å¤šæ¬¡é‡‡æ ·â†’é€‰æœ€ä¸€è‡´ç»“æœï¼šé€‚åˆç”Ÿæˆå¤šä¸ª prompt è‰æ¡ˆåæŠ•ç¥¨æ‹©ä¼˜"
    },
    {
        "id": "LEAST_TO_MOST",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2205.10625",
        "topic": "prompting",
        "tags": ["decomposition", "clarify", "workflow"],
        "notes": "é€‚åˆæŠŠæ¨¡ç³Šéœ€æ±‚æ‹†æˆå­é—®é¢˜ï¼šç›®æ ‡â†’å—ä¼—â†’æ ¼å¼â†’çº¦æŸâ†’å·¥å…·"
    },
    {
        "id": "REACT",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2210.03629",
        "topic": "prompting",
        "tags": ["react", "tool_use", "agent", "trajectories"],
        "notes": "å½“ä½ è¦åšå¸¦å·¥å…·çš„æç¤ºè¯ç”Ÿæˆå™¨/Agentæ—¶ï¼ŒReAct æä¾›æ¨¡æ¿åŒ–è½¨è¿¹"
    },
    {
        "id": "TREE_OF_THOUGHTS",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2305.10601",
        "topic": "prompting",
        "tags": ["tot", "search", "self_eval", "candidate_generation"],
        "notes": "é€‚åˆï¼šç”Ÿæˆ 2-4 ä¸ªå€™é€‰æç¤ºè¯â†’è‡ªè¯„â†’å›æº¯ä¿®æ­£"
    },
    # P0 - è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯
    {
        "id": "OPRO",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2309.03409",
        "topic": "prompt_optimization",
        "tags": ["opro", "optimization", "iteration", "evaluate_loop"],
        "notes": "æŠŠæç¤ºè¯æ”¹å†™å˜æˆè¿­ä»£ä¼˜åŒ–ï¼šå€™é€‰â†’è¯„ä¼°â†’åé¦ˆâ†’å†ç”Ÿæˆ"
    },
    {
        "id": "PROTEGI",
        "type": "paper",
        "priority": "P1",
        "url": "https://arxiv.org/abs/2305.03495",
        "topic": "prompt_optimization",
        "tags": ["protegi", "textual_gradients", "optimization"],
        "notes": "ç”¨æ–‡æœ¬æ¢¯åº¦/æ‰¹è¯„â†’å±€éƒ¨ä¿®å¤è¿­ä»£æ”¹ prompt"
    },
    {
        "id": "DSPY_README",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/stanfordnlp/dspy/main/README.md",
        "topic": "prompt_optimization",
        "tags": ["dspy", "compile", "optimizer", "evaluation"],
        "notes": "æŠŠ prompt å½“å¯ç¼–è¯‘ç¨‹åº + ä¼˜åŒ–å™¨"
    },
    # P0 - RAG æ¡†æ¶
    {
        "id": "LLAMAINDEX_README",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/run-llama/llama_index/main/README.md",
        "topic": "rag",
        "tags": ["rag", "indexing", "retrieval", "rerank"],
        "notes": "ç´¢å¼•/æ£€ç´¢/é‡æ’/æŸ¥è¯¢å¼•æ“/Agent å·¥å…·åŒ–"
    },
    # P0 - è¯„æµ‹
    {
        "id": "PROMPTFOO_README",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/promptfoo/promptfoo/main/README.md",
        "topic": "eval",
        "tags": ["eval", "regression", "red_team", "ci"],
        "notes": "æŠŠç”Ÿæˆå™¨è¾“å‡ºå½“å¯æµ‹è¯•å·¥ä»¶ï¼šæ ¼å¼éµå¾ªã€ä¸€è‡´æ€§ã€æ³¨å…¥ç”¨ä¾‹ã€å›å½’"
    },
    # P0 - ç»“æ„åŒ–è¾“å‡º
    {
        "id": "GUARDRAILS_README",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/guardrails-ai/guardrails/main/README.md",
        "topic": "structured_output",
        "tags": ["schema", "structured_output", "validation"],
        "notes": "è§£å†³ï¼šJSON/è¡¨æ ¼å­—æ®µè·‘åï¼›å¤±è´¥æ—¶è§¦å‘ä¿®å¤/å†é—®"
    },
    {
        "id": "GUIDANCE_README",
        "type": "repo",
        "priority": "P0",
        "url": "https://raw.githubusercontent.com/guidance-ai/guidance/main/README.md",
        "topic": "structured_output",
        "tags": ["constrained_decoding", "regex", "cfg", "structured_output"],
        "notes": "å½“ä½ éœ€è¦å¼ºåˆ¶æ ¼å¼ï¼ˆJSON/DSLï¼‰æ—¶ï¼Œconstrained decoding æ˜¯ç¡¬æ­¦å™¨"
    },
    # P0 - å®‰å…¨
    {
        "id": "PROMPT_INJECTION_HOUYI",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2306.05499",
        "topic": "security",
        "tags": ["prompt_injection", "security", "defense"],
        "notes": "çœŸå®åº”ç”¨ä¸­çš„æ³¨å…¥æ”»å‡»æ‹†è§£ï¼›RAG/Agent å¿…è¯»"
    },
    {
        "id": "INDIRECT_PROMPT_INJECTION",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2302.12173",
        "topic": "security",
        "tags": ["indirect_injection", "rag_security", "data_instruction_confusion"],
        "notes": "RAG æ£€ç´¢åˆ°çš„ç½‘é¡µ/é‚®ä»¶/æ–‡æ¡£é‡Œå¤¹å¸¦æŒ‡ä»¤â†’åŠ«æŒ Agent çš„ç»å…¸è·¯å¾„"
    },
    {
        "id": "INJECAGENT",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2403.02691",
        "topic": "security",
        "tags": ["benchmark", "agent_security", "indirect_injection"],
        "notes": "ç»™ä½ çš„ç”Ÿæˆå™¨åŠ æ³¨å…¥å›å½’æµ‹è¯•é›†çš„å‚è€ƒæ ‡å‡†"
    },
    {
        "id": "SYSTEM_PROMPT_POISONING",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2505.06493",
        "topic": "security",
        "tags": ["system_prompt", "poisoning", "persistence"],
        "notes": "ç³»ç»Ÿæç¤ºä¸€æ—¦è¢«æ±¡æŸ“ä¼šæŒä¹…å½±å“åç»­äº¤äº’"
    },
    # P0 - æŒ‡ä»¤å±‚çº§
    {
        "id": "IHEVAL",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2502.08745",
        "topic": "instruction_hierarchy",
        "tags": ["instruction_hierarchy", "eval", "safety"],
        "notes": "æ•™ä¼šç³»ç»Ÿï¼šSYSTEM/DEVELOPER/USER/å†å²/å·¥å…·è¾“å‡ºå†²çªæ—¶å¦‚ä½•åˆ¤å®š"
    },
    # P1 - æ¨¡æ¿åº“
    {
        "id": "FABRIC_README",
        "type": "repo",
        "priority": "P1",
        "url": "https://raw.githubusercontent.com/danielmiessler/fabric/main/README.md",
        "topic": "prompting",
        "tags": ["patterns", "template_library", "standardization"],
        "notes": "å¤§é‡ patternsï¼›é€‚åˆæŠ½å–ç»Ÿä¸€ç»“æ„"
    },
    {
        "id": "DAIR_PROMPTING_GUIDE",
        "type": "repo",
        "priority": "P1",
        "url": "https://raw.githubusercontent.com/dair-ai/Prompt-Engineering-Guide/main/README.md",
        "topic": "prompting",
        "tags": ["guide", "survey", "prompting", "rag", "agents"],
        "notes": "è¦†ç›– prompting/RAG/agents/æŠ€å·§åˆé›†"
    },
    # Prompt Pattern Catalog
    {
        "id": "PROMPT_PATTERN_CATALOG",
        "type": "paper",
        "priority": "P0",
        "url": "https://arxiv.org/abs/2302.11382",
        "topic": "prompting",
        "tags": ["patterns", "prompt_components", "persona", "workflow", "format"],
        "notes": "æŠŠæç¤ºè¯å†™ä½œæŠ½è±¡ä¸ºå¯å¤ç”¨ Patternï¼šPersona/Recipe/Format/Refusal/â€¦"
    },
]

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(__file__).parent.parent / "data" / "corpus_cache"


async def fetch_url(client: httpx.AsyncClient, url: str) -> Optional[str]:
    """è·å– URL å†…å®¹"""
    try:
        # å¤„ç† arXiv è®ºæ–‡ - è·å–æ‘˜è¦é¡µ
        if "arxiv.org/abs/" in url:
            response = await client.get(url, follow_redirects=True, timeout=30)
            if response.status_code == 200:
                return response.text
        else:
            response = await client.get(url, follow_redirects=True, timeout=30)
            if response.status_code == 200:
                return response.text
        return None
    except Exception as e:
        print(f"  âŒ è·å–å¤±è´¥: {url} - {e}")
        return None


def extract_arxiv_abstract(html: str) -> str:
    """ä» arXiv é¡µé¢æå–æ‘˜è¦"""
    # ç®€å•çš„æ­£åˆ™æå–
    abstract_match = re.search(r'<blockquote class="abstract[^"]*">\s*<span class="descriptor">Abstract:</span>\s*(.*?)</blockquote>', html, re.DOTALL)
    if abstract_match:
        abstract = abstract_match.group(1).strip()
        # æ¸…ç† HTML æ ‡ç­¾
        abstract = re.sub(r'<[^>]+>', '', abstract)
        return abstract
    return ""


def extract_title(html: str, url: str) -> str:
    """æå–é¡µé¢æ ‡é¢˜"""
    title_match = re.search(r'<title>([^<]+)</title>', html, re.IGNORECASE)
    if title_match:
        return title_match.group(1).strip()
    return url.split("/")[-1]


def clean_html(html: str) -> str:
    """æ¸…ç† HTMLï¼Œæå–çº¯æ–‡æœ¬"""
    # ç§»é™¤ script å’Œ style
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    # ç§»é™¤ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', ' ', html)
    # æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    """å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå—"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†
        if end < len(text):
            for sep in ['\n\n', '\n', 'ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', 'ï¼›', ';']:
                last_sep = text.rfind(sep, start, end)
                if last_sep > start + chunk_size // 2:
                    end = last_sep + len(sep)
                    break
        
        chunk = text[start:end].strip()
        if chunk and len(chunk) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å—
            chunks.append(chunk)
        
        start = end - overlap
    
    return chunks


async def crawl_all():
    """çˆ¬å–æ‰€æœ‰ URL"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    all_documents = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    async with httpx.AsyncClient(headers=headers) as client:
        for item in CORPUS_URLS:
            url = item["url"]
            doc_id = item["id"]
            doc_type = item["type"]
            
            print(f"ğŸ“¥ æ­£åœ¨è·å–: {doc_id} ({doc_type})")
            print(f"   URL: {url}")
            
            content = await fetch_url(client, url)
            
            if not content:
                print(f"   âš ï¸ è·³è¿‡ï¼ˆæ— æ³•è·å–ï¼‰")
                continue
            
            # æ ¹æ®ç±»å‹å¤„ç†å†…å®¹
            if doc_type == "paper" and "arxiv.org" in url:
                # arXiv è®ºæ–‡ - æå–æ‘˜è¦
                abstract = extract_arxiv_abstract(content)
                title = extract_title(content, url)
                
                if abstract:
                    processed_content = f"# {title}\n\n## Abstract\n{abstract}\n\n## Notes\n{item['notes']}"
                else:
                    processed_content = f"# {title}\n\n## Notes\n{item['notes']}"
            elif doc_type == "repo":
                # GitHub README - ç›´æ¥ä½¿ç”¨
                processed_content = content
            else:
                # æ–‡æ¡£é¡µé¢ - æ¸…ç† HTML
                title = extract_title(content, url)
                text = clean_html(content)
                processed_content = f"# {title}\n\n{text[:5000]}"  # é™åˆ¶é•¿åº¦
            
            # åˆ‡å—
            chunks = chunk_text(processed_content)
            
            print(f"   âœ… è·å–æˆåŠŸï¼Œåˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
            
            # ä¿å­˜åˆ°æœ¬åœ°
            cache_file = OUTPUT_DIR / f"{doc_id}.json"
            cache_data = {
                "id": doc_id,
                "url": url,
                "type": doc_type,
                "priority": item["priority"],
                "topic": item["topic"],
                "tags": item["tags"],
                "notes": item["notes"],
                "content": processed_content,
                "chunks": chunks,
                "fetched_at": datetime.now().isoformat()
            }
            
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            for i, chunk in enumerate(chunks):
                all_documents.append({
                    "doc_id": f"{doc_id}_chunk_{i}",
                    "content": chunk,
                    "metadata": {
                        "source_id": doc_id,
                        "source_type": doc_type,
                        "source_url": url,
                        "priority": item["priority"],
                        "topic": item["topic"],
                        "tags": ",".join(item["tags"]),
                        "notes": item["notes"],
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "type": "corpus_knowledge"
                    }
                })
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(0.5)
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_file = OUTPUT_DIR / "_all_documents.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(all_documents, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š çˆ¬å–å®Œæˆï¼")
    print(f"   æ€»æ–‡æ¡£æ•°: {len(CORPUS_URLS)}")
    print(f"   æ€»å—æ•°: {len(all_documents)}")
    print(f"   ç¼“å­˜ç›®å½•: {OUTPUT_DIR}")
    
    return all_documents


if __name__ == "__main__":
    asyncio.run(crawl_all())
