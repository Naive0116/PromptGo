{
  "id": "GUARDRAILS_README",
  "url": "https://raw.githubusercontent.com/guardrails-ai/guardrails/main/README.md",
  "type": "repo",
  "priority": "P0",
  "topic": "structured_output",
  "tags": [
    "schema",
    "structured_output",
    "validation"
  ],
  "notes": "解决：JSON/表格字段跑偏；失败时触发修复/再问",
  "content": "<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/Guardrails-ai-logo-for-dark-bg.svg#gh-dark-mode-only\" alt=\"Guardrails AI Logo\" width=\"600px\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/Guardrails-ai-logo-for-white-bg.svg#gh-light-mode-only\" alt=\"Guardrails AI Logo\" width=\"600px\">\n\n<hr>\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/guardrails-ai)\n[![Downloads](https://static.pepy.tech/badge/guardrails-ai/month)](https://pepy.tech/project/guardrails-ai)\n[![CI](https://github.com/guardrails-ai/guardrails/actions/workflows/ci.yml/badge.svg)](https://github.com/guardrails-ai/guardrails/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/guardrails-ai/guardrails/graph/badge.svg?token=CPkjw91Ngo)](https://codecov.io/gh/guardrails-ai/guardrails)\n[![Checked with pyright](https://microsoft.github.io/pyright/img/pyright_badge.svg)](https://microsoft.github.io/pyright/)\n[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/guardrails_ai)](https://x.com/guardrails_ai)\n[![Discord](https://img.shields.io/discord/1085077079697150023?logo=discord&label=support&link=https%3A%2F%2Fdiscord.gg%2Fgw4cR9QvYE)](https://discord.gg/U9RKkZSBgx)\n[![Static Badge](https://img.shields.io/badge/Docs-blue?link=https%3A%2F%2Fwww.guardrailsai.com%2Fdocs)](https://www.guardrailsai.com/docs)\n[![Static Badge](https://img.shields.io/badge/Blog-blue?link=https%3A%2F%2Fwww.guardrailsai.com%2Fblog)](https://www.guardrailsai.com/blog)\n[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20Guardrails%20Guru-006BFF)](https://gurubase.io/g/guardrails)\n\n</div>\n\n## News and Updates\n- **[Feb 12, 2025]** We just launched Guardrails Index -- the first of its kind benchmark comparing the performance and latency of 24 guardrails across 6 most common categories! Check out the index at index.guardrailsai.com\n\n## What is Guardrails?\n\nGuardrails is a Python framework that helps build reliable AI applications by performing two key functions:\n1. Guardrails runs Input/Output Guards in your application that detect, quantify and mitigate the presence of specific types of risks. To look at the full suite of risks, check out [Guardrails Hub](https://hub.guardrailsai.com/).\n2. Guardrails help you generate structured data from LLMs.\n\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/with_and_without_guardrails.svg\" alt=\"Guardrails in your application\" width=\"1500px\">\n</div>\n\n\n### Guardrails Hub\n\nGuardrails Hub is a collection of pre-built measures of specific types of risks (called 'validators'). Multiple validators can be combined together into Input and Output Guards that intercept the inputs and outputs of LLMs. Visit [Guardrails Hub](https://hub.guardrailsai.com/) to see the full list of validators and their documentation.\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/guardrails_hub.gif\" alt=\"Guardrails Hub gif\" width=\"600px\">\n</div>\n\n\n## Installation\n\n```python\npip install guardrails-ai\n```\n\n\n## Getting Started\n\n\n### Create Input and Output Guards for LLM Validation\n\n1. Download and configure the Guardrails Hub CLI.\n\n    ```bash\n    pip install guardrails-ai\n    guardrails configure\n    ```\n2. Install a guardrail from Guardrails Hub.\n\n    ```bash\n    guardrails hub install hub://guardrails/regex_match\n    ```\n3. Create a Guard from the installed guardrail.\n\n    ```python\n    from guardrails import Guard, OnFailAction\n    from guardrails.hub import RegexMatch\n\n    guard = Guard().use(\n        RegexMatch, regex=\"\\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\", on_fail=OnFailAction.EXCEPTION\n    )\n\n    guard.validate(\"123-456-7890\")  # Guardrail passes\n\n    try:\n        guard.validate(\"1234-789-0000\")  # Guardrail fails\n    except Exception as e:\n        print(e)\n    ```\n    Output:\n    ```console\n    Validation failed for field with errors: Result must match \\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\n    ```\n4. Run multiple guardrails within a Guard.\n    First, install the necessary guardrails from Guardrails Hub.\n\n    ```bash\n    guardrails hub install hub://guardrails/competitor_check\n    guardrails hub install hub://guardrails/toxic_language\n    ```\n\n    Then, create a Guard from the installed guardrails.\n\n    ```python\n    from guardrails import Guard, OnFailAction\n    from guardrails.hub import CompetitorCheck, ToxicLanguage\n\n    guard = Guard().use_many(\n        CompetitorCheck([\"Apple\", \"Microsoft\", \"Google\"], on_fail=OnFailAction.EXCEPTION),\n        ToxicLanguage(threshold=0.5, validation_method=\"sentence\", on_fail=OnFailAction.EXCEPTION)\n    )\n\n    guard.validate(\n        \"\"\"An apple a day keeps a doctor away.\n        This is good advice for keeping your health.\"\"\"\n    )  # Both the guardrails pass\n\n    try:\n        guard.validate(\n            \"\"\"Shut the hell up! Apple just released a new iPhone.\"\"\"\n        )  # Both the guardrails fail\n    except Exception as e:\n        print(e)\n    ```\n    Output:\n    ```console\n    Validation failed for field with errors: Found the following competitors: [['Apple']]. Please avoid naming those competitors next time, The following sentences in your response were found to be toxic:\n\n    - Shut the hell up!\n    ```\n\n### Use Guardrails to generate structured data from LLMs\n\n\nLet's go through an example where we ask an LLM to generate fake pet names. To do this, we'll create a Pydantic [BaseModel](https://docs.pydantic.dev/latest/api/base_model/) that represents the structure of the output we want.\n\n```py\nfrom pydantic import BaseModel, Field\n\nclass Pet(BaseModel):\n    pet_type: str = Field(description=\"Species of pet\")\n    name: str = Field(description=\"a unique pet name\")\n```\n\nNow, create a Guard from the `Pet` class. The Guard can be used to call the LLM in a manner so that the output is formatted to the `Pet` class. Under the hood, this is done by either of two methods:\n1. Function calling: For LLMs that support function calling, we generate structured data using the function call syntax.\n2. Prompt optimization: For LLMs that don't support function calling, we add the schema of the expected output to the prompt so that the LLM can generate structured data.\n\n```py\nfrom guardrails import Guard\nimport openai\n\nprompt = \"\"\"\n    What kind of pet should I get and what should I name it?\n\n    ${gr.complete_json_suffix_v2}\n\"\"\"\nguard = Guard.for_pydantic(output_class=Pet, prompt=prompt)\n\nraw_output, validated_output, *rest = guard(\n    llm_api=openai.completions.create,\n    engine=\"gpt-3.5-turbo-instruct\"\n)\n\nprint(validated_output)\n```\n\nThis prints:\n```\n{\n    \"pet_type\": \"dog\",\n    \"name\": \"Buddy\n}\n```\n\n### Guardrails Server\n\nGuardrails can be set up as a standalone service served by Flask with `guardrails start`, allowing you to interact with it via a REST API. This approach simplifies development and deployment of Guardrails-powered applications.\n\n1. Install: `pip install \"guardrails-ai\"`\n2. Configure: `guardrails configure`\n3. Create a config: `guardrails create --validators=hub://guardrails/two_words --guard-name=two-word-guard`\n4. Start the dev server: `guardrails start --config=./config.py`\n5. Interact with the dev server via the snippets below\n```\n# with the guardrails client\nimport guardrails as gr\n\ngr.settings.use_server = True\nguard = gr.Guard(name='two-word-guard')\nguard.validate('this is more than two words')\n\n# or with the openai sdk\nimport openai\nopenai.base_url = \"http://localhost:8000/guards/two-word-guard/openai/v1/\"\nos.environ[\"OPENAI_API_KEY\"] = \"youropenaikey\"\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"tell me about an apple with 3 words exactly\",\n        },\n    ]\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n)\n```\n\nFor production deployments, we recommend using Docker with Gunicorn as the WSGI server for improved performance and scalability.\n\n## FAQ\n\n#### I'm running into issues with Guardrails. Where can I get help?\n\nYou can reach out to us on [Discord](https://discord.gg/gw4cR9QvYE) or [Twitter](https://twitter.com/guardrails_ai).\n\n#### Can I use Guardrails with any LLM?\n\nYes, Guardrails can be used with proprietary and open-source LLMs. Check out this guide on [how to use Guardrails with any LLM](https://www.guardrailsai.com/docs/how_to_guides/llm_api_wrappers).\n\n#### Can I create my own validators?\n\nYes, you can create your own validators and contribute them to Guardrails Hub. Check out this guide on [how to create your own validators](https://www.guardrailsai.com/docs/hub/how_to_guides/custom_validator).\n\n#### Does Guardrails support other languages?\n\nGuardrails can be used with Python and JavaScript. Check out the docs on how to use Guardrails from JavaScript. We are working on adding support for other languages. If you would like to contribute to Guardrails, please reach out to us on [Discord](https://discord.gg/gw4cR9QvYE) or [Twitter](https://twitter.com/guardrails_ai).\n\n\n## Contributing\n\nWe welcome contributions to Guardrails!\n\nGet started by checking out Github issues and check out the [Contributing Guide](CONTRIBUTING.md). Feel free to open an issue, or reach out if you would like to add to the project!\n",
  "chunks": [
    "<div align=\"center\">\n\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/Guardrails-ai-logo-for-dark-bg.svg#gh-dark-mode-only\" alt=\"Guardrails AI Logo\" width=\"600px\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/Guardrails-ai-logo-for-white-bg.svg#gh-light-mode-only\" alt=\"Guardrails AI Logo\" width=\"600px\">\n\n<hr>\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/guardrails-ai)\n[![Downloads](https://static.pepy.tech/badge/guardrails-ai/month)](https://pepy.tech/project/guardrails-ai)",
    "oads](https://static.pepy.tech/badge/guardrails-ai/month)](https://pepy.tech/project/guardrails-ai)\n[![CI](https://github.com/guardrails-ai/guardrails/actions/workflows/ci.yml/badge.svg)](https://github.com/guardrails-ai/guardrails/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/guardrails-ai/guardrails/graph/badge.svg?token=CPkjw91Ngo)](https://codecov.io/gh/guardrails-ai/guardrails)\n[![Checked with pyright](https://microsoft.github.io/pyright/img/pyright_badge.svg)](https://microsoft.github.io/pyright/)\n[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/guardrails_ai)](https://x.com/guardrails_ai)",
    "Twitter) Follow](https://img.shields.io/twitter/follow/guardrails_ai)](https://x.com/guardrails_ai)\n[![Discord](https://img.shields.io/discord/1085077079697150023?logo=discord&label=support&link=https%3A%2F%2Fdiscord.gg%2Fgw4cR9QvYE)](https://discord.gg/U9RKkZSBgx)\n[![Static Badge](https://img.shields.io/badge/Docs-blue?link=https%3A%2F%2Fwww.guardrailsai.com%2Fdocs)](https://www.guardrailsai.com/docs)\n[![Static Badge](https://img.shields.io/badge/Blog-blue?link=https%3A%2F%2Fwww.guardrailsai.com%2Fblog)](https://www.guardrailsai.com/blog)\n[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20Guardrails%20Guru-006BFF)](https://gurubase.io/g/guardrails)\n\n</div>",
    "ields.io/badge/Gurubase-Ask%20Guardrails%20Guru-006BFF)](https://gurubase.io/g/guardrails)\n\n</div>\n\n## News and Updates\n- **[Feb 12, 2025]** We just launched Guardrails Index -- the first of its kind benchmark comparing the performance and latency of 24 guardrails across 6 most common categories! Check out the index at index.guardrailsai.com\n\n## What is Guardrails?\n\nGuardrails is a Python framework that helps build reliable AI applications by performing two key functions:\n1. Guardrails runs Input/Output Guards in your application that detect, quantify and mitigate the presence of specific types of risks. To look at the full suite of risks, check out [Guardrails Hub](https://hub.guardrailsai.com/).\n2. Guardrails help you generate structured data from LLMs.",
    "s Hub](https://hub.guardrailsai.com/).\n2. Guardrails help you generate structured data from LLMs.\n\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/with_and_without_guardrails.svg\" alt=\"Guardrails in your application\" width=\"1500px\">\n</div>\n\n\n### Guardrails Hub\n\nGuardrails Hub is a collection of pre-built measures of specific types of risks (called 'validators'). Multiple validators can be combined together into Input and Output Guards that intercept the inputs and outputs of LLMs. Visit [Guardrails Hub](https://hub.guardrailsai.com/) to see the full list of validators and their documentation.",
    "ls Hub](https://hub.guardrailsai.com/) to see the full list of validators and their documentation.\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/guardrails-ai/guardrails/main/docs/dist/img/guardrails_hub.gif\" alt=\"Guardrails Hub gif\" width=\"600px\">\n</div>\n\n\n## Installation\n\n```python\npip install guardrails-ai\n```\n\n\n## Getting Started\n\n\n### Create Input and Output Guards for LLM Validation\n\n1. Download and configure the Guardrails Hub CLI.\n\n    ```bash\n    pip install guardrails-ai\n    guardrails configure\n    ```\n2. Install a guardrail from Guardrails Hub.\n\n    ```bash\n    guardrails hub install hub://guardrails/regex_match\n    ```\n3. Create a Guard from the installed guardrail.",
    "s hub install hub://guardrails/regex_match\n    ```\n3. Create a Guard from the installed guardrail.\n\n    ```python\n    from guardrails import Guard, OnFailAction\n    from guardrails.hub import RegexMatch\n\n    guard = Guard().use(\n        RegexMatch, regex=\"\\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\", on_fail=OnFailAction.EXCEPTION\n    )\n\n    guard.validate(\"123-456-7890\")  # Guardrail passes\n\n    try:\n        guard.validate(\"1234-789-0000\")  # Guardrail fails\n    except Exception as e:\n        print(e)\n    ```\n    Output:\n    ```console\n    Validation failed for field with errors: Result must match \\(?\\d{3}\\)?-? *\\d{3}-? *-?\\d{4}\n    ```\n4. Run multiple guardrails within a Guard.\n    First, install the necessary guardrails from Guardrails Hub.",
    "ltiple guardrails within a Guard.\n    First, install the necessary guardrails from Guardrails Hub.\n\n    ```bash\n    guardrails hub install hub://guardrails/competitor_check\n    guardrails hub install hub://guardrails/toxic_language\n    ```\n\n    Then, create a Guard from the installed guardrails.\n\n    ```python\n    from guardrails import Guard, OnFailAction\n    from guardrails.hub import CompetitorCheck, ToxicLanguage\n\n    guard = Guard().use_many(\n        CompetitorCheck([\"Apple\", \"Microsoft\", \"Google\"], on_fail=OnFailAction.EXCEPTION),\n        ToxicLanguage(threshold=0.5, validation_method=\"sentence\", on_fail=OnFailAction.EXCEPTION)\n    )",
    "ToxicLanguage(threshold=0.5, validation_method=\"sentence\", on_fail=OnFailAction.EXCEPTION)\n    )\n\n    guard.validate(\n        \"\"\"An apple a day keeps a doctor away.\n        This is good advice for keeping your health.\"\"\"\n    )  # Both the guardrails pass\n\n    try:\n        guard.validate(\n            \"\"\"Shut the hell up! Apple just released a new iPhone.\"\"\"\n        )  # Both the guardrails fail\n    except Exception as e:\n        print(e)\n    ```\n    Output:\n    ```console\n    Validation failed for field with errors: Found the following competitors: [['Apple']]. Please avoid naming those competitors next time, The following sentences in your response were found to be toxic:\n\n    - Shut the hell up!\n    ```\n\n### Use Guardrails to generate structured data from LLMs",
    "toxic:\n\n    - Shut the hell up!\n    ```\n\n### Use Guardrails to generate structured data from LLMs\n\n\nLet's go through an example where we ask an LLM to generate fake pet names. To do this, we'll create a Pydantic [BaseModel](https://docs.pydantic.dev/latest/api/base_model/) that represents the structure of the output we want.\n\n```py\nfrom pydantic import BaseModel, Field\n\nclass Pet(BaseModel):\n    pet_type: str = Field(description=\"Species of pet\")\n    name: str = Field(description=\"a unique pet name\")\n```",
    "r = Field(description=\"Species of pet\")\n    name: str = Field(description=\"a unique pet name\")\n```\n\nNow, create a Guard from the `Pet` class. The Guard can be used to call the LLM in a manner so that the output is formatted to the `Pet` class. Under the hood, this is done by either of two methods:\n1. Function calling: For LLMs that support function calling, we generate structured data using the function call syntax.\n2. Prompt optimization: For LLMs that don't support function calling, we add the schema of the expected output to the prompt so that the LLM can generate structured data.\n\n```py\nfrom guardrails import Guard\nimport openai\n\nprompt = \"\"\"\n    What kind of pet should I get and what should I name it?",
    "ort Guard\nimport openai\n\nprompt = \"\"\"\n    What kind of pet should I get and what should I name it?\n\n    ${gr.complete_json_suffix_v2}\n\"\"\"\nguard = Guard.for_pydantic(output_class=Pet, prompt=prompt)\n\nraw_output, validated_output, *rest = guard(\n    llm_api=openai.completions.create,\n    engine=\"gpt-3.5-turbo-instruct\"\n)\n\nprint(validated_output)\n```\n\nThis prints:\n```\n{\n    \"pet_type\": \"dog\",\n    \"name\": \"Buddy\n}\n```\n\n### Guardrails Server\n\nGuardrails can be set up as a standalone service served by Flask with `guardrails start`, allowing you to interact with it via a REST API. This approach simplifies development and deployment of Guardrails-powered applications.",
    "REST API. This approach simplifies development and deployment of Guardrails-powered applications.\n\n1. Install: `pip install \"guardrails-ai\"`\n2. Configure: `guardrails configure`\n3. Create a config: `guardrails create --validators=hub://guardrails/two_words --guard-name=two-word-guard`\n4. Start the dev server: `guardrails start --config=./config.py`\n5. Interact with the dev server via the snippets below\n```\n# with the guardrails client\nimport guardrails as gr\n\ngr.settings.use_server = True\nguard = gr.Guard(name='two-word-guard')\nguard.validate('this is more than two words')\n\n# or with the openai sdk\nimport openai\nopenai.base_url = \"http://localhost:8000/guards/two-word-guard/openai/v1/\"\nos.environ[\"OPENAI_API_KEY\"] = \"youropenaikey\"",
    "://localhost:8000/guards/two-word-guard/openai/v1/\"\nos.environ[\"OPENAI_API_KEY\"] = \"youropenaikey\"\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"tell me about an apple with 3 words exactly\",\n        },\n    ]\n\ncompletion = openai.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n)\n```\n\nFor production deployments, we recommend using Docker with Gunicorn as the WSGI server for improved performance and scalability.\n\n## FAQ\n\n#### I'm running into issues with Guardrails. Where can I get help?\n\nYou can reach out to us on [Discord](https://discord.gg/gw4cR9QvYE) or [Twitter](https://twitter.com/guardrails_ai).\n\n#### Can I use Guardrails with any LLM?",
    "cR9QvYE) or [Twitter](https://twitter.com/guardrails_ai).\n\n#### Can I use Guardrails with any LLM?\n\nYes, Guardrails can be used with proprietary and open-source LLMs. Check out this guide on [how to use Guardrails with any LLM](https://www.guardrailsai.com/docs/how_to_guides/llm_api_wrappers).\n\n#### Can I create my own validators?\n\nYes, you can create your own validators and contribute them to Guardrails Hub. Check out this guide on [how to create your own validators](https://www.guardrailsai.com/docs/hub/how_to_guides/custom_validator).\n\n#### Does Guardrails support other languages?",
    "ilsai.com/docs/hub/how_to_guides/custom_validator).\n\n#### Does Guardrails support other languages?\n\nGuardrails can be used with Python and JavaScript. Check out the docs on how to use Guardrails from JavaScript. We are working on adding support for other languages. If you would like to contribute to Guardrails, please reach out to us on [Discord](https://discord.gg/gw4cR9QvYE) or [Twitter](https://twitter.com/guardrails_ai).\n\n\n## Contributing\n\nWe welcome contributions to Guardrails!\n\nGet started by checking out Github issues and check out the [Contributing Guide](CONTRIBUTING.md). Feel free to open an issue, or reach out if you would like to add to the project!"
  ],
  "fetched_at": "2026-01-27T18:24:59.095550"
}